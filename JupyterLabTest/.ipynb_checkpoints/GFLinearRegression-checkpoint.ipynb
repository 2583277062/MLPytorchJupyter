{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ef0291d5-6398-437d-8d0c-681f9572e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "df41b4d6-c45e-4b12-9a28-55e70641cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./GF1_cleaned.csv')\n",
    "df.corr()['load'].sort_values(ascending=False)\n",
    "features = df.loc[:,['swr','T2m','T']]\n",
    "labels = df.loc[:,'load']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bdbd0014-a476-45b4-9504-5169588efe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0,num_examples,batch_size):\n",
    "        batch_indices = torch.tensor(indices[i:min(i+ batch_size,num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "62c785da-34ed-4d71-9090-c68507756aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38686, 3])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = torch.tensor(features.values).to(torch.float32)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5d97c6a6-6d32-4a8f-8e89-692b7e1a44fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38686])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.tensor(labels.values).to(torch.float32)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e692c873-7c54-4e0a-bc31-be22b1e4c1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8700e+00, 2.7663e+02, 2.7674e+02],\n",
      "        [7.4000e-01, 2.9397e+02, 2.9868e+02],\n",
      "        [4.4698e+02, 2.9411e+02, 2.9266e+02],\n",
      "        [1.9100e+00, 2.6149e+02, 2.6559e+02],\n",
      "        [2.6600e+00, 2.5933e+02, 2.6221e+02],\n",
      "        [6.9888e+02, 3.0672e+02, 3.0540e+02],\n",
      "        [1.1200e+00, 2.9118e+02, 2.9265e+02],\n",
      "        [5.1000e-01, 2.8512e+02, 2.8820e+02],\n",
      "        [1.1270e+01, 2.8485e+02, 2.8515e+02],\n",
      "        [2.0300e+00, 2.7002e+02, 2.6965e+02]], dtype=torch.float64) \n",
      " tensor([[0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [2.7699e+01],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [4.0225e+01],\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [2.8000e-03],\n",
      "        [0.0000e+00]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for X,y in data_iter(10, features,labels):\n",
    "    print(X,'\\n',y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "128395c4-7c60-4061-8067-1243ee37b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.normal(0, 0.01, size=(3,1), requires_grad=True).to(torch.float32)\n",
    "b = torch.zeros(1, requires_grad=True).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c249bb2e-a437-4f82-adcb-b5d9152b6ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0062],\n",
       "        [-0.0155],\n",
       "        [-0.0073]], dtype=torch.float64, grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "89a4f327-da8f-4774-a668-94882861cf3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], dtype=torch.float64, grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7e41e8af-c4f6-4286-b36f-fe749d92eadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b):  #@save\n",
    "    \"\"\"线性回归模型。\"\"\"\n",
    "    return torch.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "eb6e6a22-a95d-4329-89a7-aefa3edc4a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"均方损失。\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e1705aa1-112a-48b8-a220-9e2a480c4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params,lr,batch_size):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad/batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b2379347-99e8-4796-99b7-d36d470b71e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25832\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24220/3281732109.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m#         print(f'epoch{epoch}, w:{w},b:{b}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24220/3502029650.py\u001b[0m in \u001b[0;36msgd\u001b[1;34m(params, lr, batch_size)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m             \u001b[0mparam\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m             \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "lr = 0.01\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in data_iter(20,features,labels):\n",
    "        l = loss(net(X,w,b),y)\n",
    "        l.sum().backward()\n",
    "        sgd([w,b],lr,20)\n",
    "#         print(f'epoch{epoch}, w:{w},b:{b}')\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features,w,b),labels)\n",
    "        print(f'epoch:{epoch},loss:{float(train_l.mean())}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
